{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPA1QSwZI8i2KTk6S3ZUpVp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TonKphumpl/NutriLedger-App/blob/master/Cleaning_script_VisualETL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkGiVwiqqpMi"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from awsglue.utils import getResolvedOptions\n",
        "from pyspark.context import SparkContext\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.job import Job\n",
        "\n",
        "import os\n",
        "import awswrangler as wr\n",
        "import pandas as pd\n",
        "import pymysql\n",
        "from datetime import datetime\n",
        "\n",
        "# Glue job parameters\n",
        "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
        "\n",
        "def main():\n",
        "    # Init Glue contexts\n",
        "    sc = SparkContext()\n",
        "    glueContext = GlueContext(sc)\n",
        "    job = Job(glueContext)\n",
        "    job.init(args['JOB_NAME'], args)\n",
        "\n",
        "    # Config\n",
        "    s3_base_path = os.getenv('S3_BASE_PATH', 's3://income-expense-tracker-webapp/user_data/')\n",
        "    db_table = os.getenv('DB_TABLE', 'user_data')\n",
        "    db_connection = os.getenv('DB_CONNECTION', 'jdbc_connection')\n",
        "    db_schema = os.getenv('DB_SCHEMA', 'nutri')\n",
        "\n",
        "    print(f\"üì• Reading dataset from S3: {s3_base_path}\")\n",
        "    try:\n",
        "        df = wr.s3.read_csv(path=s3_base_path, sep=\",\", header=0)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to read data from S3. Error: {str(e)}\")\n",
        "        job.commit()\n",
        "        return\n",
        "\n",
        "    if df.empty:\n",
        "        print(f\"‚ö†Ô∏è No data rows found.\")\n",
        "        job.commit()\n",
        "        return\n",
        "\n",
        "    print(f\"‚úÖ Loaded {len(df)} rows.\")\n",
        "\n",
        "    # ‚úÖ Rename columns to match MySQL table\n",
        "    df.rename(columns={\n",
        "        'Date': 'date_',\n",
        "        'Type': 'type_',\n",
        "        'Expense category': 'expense_category',\n",
        "        'Lists': 'lists',\n",
        "        'Amount': 'amount',\n",
        "        'Menu': 'menu',\n",
        "        'Calories': 'calories'\n",
        "    }, inplace=True)\n",
        "\n",
        "    # --- üßπ Data Cleaning ---\n",
        "    df['date_'] = pd.to_datetime(df['date_'], errors='coerce')\n",
        "    bad_dates = df[df['date_'].isna()]\n",
        "    if not bad_dates.empty:\n",
        "        print(f\"‚ö†Ô∏è Found {len(bad_dates)} rows with invalid date values. Dropping them.\")\n",
        "    df = df.dropna(subset=['date_'])\n",
        "    df['date_'] = df['date_'].dt.date\n",
        "\n",
        "    # ‚úÖ Only keep expected columns\n",
        "    required_columns = ['date_', 'type_', 'expense_category', 'lists', 'amount', 'menu', 'calories']\n",
        "    missing_cols = [col for col in required_columns if col not in df.columns]\n",
        "    if missing_cols:\n",
        "        print(f\"‚ùå Missing columns in data: {missing_cols}\")\n",
        "        job.commit()\n",
        "        return\n",
        "    df = df[required_columns]\n",
        "\n",
        "    # Convert numeric columns\n",
        "    for col in ['amount', 'calories']:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    print(f\"üßæ DataFrame columns before write: {df.columns.tolist()}\")\n",
        "    print(f\"üî¢ Sample data:\\n{df.head()}\")\n",
        "\n",
        "    # Connect to MySQL\n",
        "    try:\n",
        "        conn = wr.mysql.connect(connection=db_connection)\n",
        "        print(f\"‚úÖ Connected to MySQL\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå MySQL connection failed. Error: {str(e)}\")\n",
        "        job.commit()\n",
        "        return\n",
        "\n",
        "    # --- üîÑ Full Load ---\n",
        "    print(f\"üóëÔ∏è Deleting all existing records from {db_schema}.{db_table}\")\n",
        "    try:\n",
        "        with conn.cursor() as cursor:\n",
        "            cursor.execute(f\"DELETE FROM {db_schema}.{db_table}\")\n",
        "        conn.commit()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Failed to delete old records. Error: {str(e)}\")\n",
        "\n",
        "    # --- ‚úÖ Write to MySQL ---\n",
        "    print(f\"üìù Writing {len(df)} cleaned rows to RDS\")\n",
        "    try:\n",
        "        wr.mysql.to_sql(\n",
        "            df=df,\n",
        "            con=conn,\n",
        "            schema=db_schema,\n",
        "            table=db_table,\n",
        "            mode='append',\n",
        "            index=False\n",
        "        )\n",
        "        print(f\"‚úÖ Full data load completed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to write data. Error: {str(e)}\")\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "    job.commit()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}